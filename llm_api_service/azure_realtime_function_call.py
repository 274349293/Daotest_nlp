import json
import re
import math
import asyncio
import time
import aiohttp
from typing import List, Optional, Dict, Any, Union
from pydantic import BaseModel
from utils.nlp_logging import CustomLogger
import jieba
import jieba.analyse
from collections import Counter

logger = CustomLogger(name="DaoTest optimized retrieval api", write_to_file=True)
"""
Azure realtime å®æ—¶è¯­éŸ³æ¨¡å‹ function call è°ƒç”¨ç»“æœè¿”å›æ¥å£
1.ç›®å‰æµ‹è¯•é˜¶æ®µæ”¯æŒä¸¤ä¸ªtopicï¼šä¼ä¸šå‡ºæµ· å’Œ æ±‡ä»åŸ¹è®­
2.æ–°å¢configï¼Œæ‰€æœ‰é…ç½®æ–‡ä»¶å‡å¯åœ¨ utils/prompt.jsonæ–‡ä»¶ä¸­é…ç½®

update:
1.250519 æ–°å¢æ–°é—»æ£€ç´¢æ¥å£ï¼Œè§¦å‘åè°ƒç”¨news.apiå»æœå¯»ç›¸å…³æ–°é—»ï¼Œç„¶ååšsummaryè¿”å›
2.ä¼˜åŒ–è¿”å›æ ¼å¼ï¼Œç»Ÿä¸€ä½¿ç”¨å›ºå®šçš„resultå­—æ®µä½œä¸ºä¸»è¦å†…å®¹è¿”å›

"""
# æ–°é—»APIé…ç½®
NEWS_API_URL = "https://eventregistry.org/api/v1/article/getArticles"
NEWS_API_KEY = "81299327-0db9-44f5-8c7c-f879c302fe8b"

# å…¨å±€æ¨¡å‹ç¼“å­˜ - é¿å…é‡å¤åŠ è½½
_GLOBAL_MODEL_CACHE = {}
# å…¨å±€ç´¢å¼•ç¼“å­˜ - æ–°å¢ï¼šç¼“å­˜å·²æ„å»ºçš„ç´¢å¼•
_GLOBAL_INDEX_CACHE = {}
# å…¨å±€å˜é‡æ§åˆ¶jiebaåŠ è½½
_JIEBA_LOADED = False


# é¢„åŠ è½½ åªæœ‰é¦–æ¬¡å¯åŠ¨è€—æ—¶
def preload_jieba():
    """é¢„åŠ è½½jiebaåˆ†è¯åº“"""
    try:
        import jieba
        import jieba.analyse
        # è§¦å‘å­—å…¸åŠ è½½
        jieba.lcut("é¢„åŠ è½½æµ‹è¯•")
        jieba.analyse.extract_tags("é¢„åŠ è½½æµ‹è¯•", topK=1)
        logger.info("Jieba preloaded successfully")
    except Exception as e:
        logger.error(f"Failed to preload jieba: {e}")


def get_cached_model(model_type: str, model_name: str = None):
    """è·å–ç¼“å­˜çš„æ¨¡å‹"""
    cache_key = f"{model_type}_{model_name}" if model_name else model_type

    if cache_key not in _GLOBAL_MODEL_CACHE:
        try:
            if model_type == "sentence_transformer":
                from sentence_transformers import SentenceTransformer
                model = SentenceTransformer("./temp_model")
                _GLOBAL_MODEL_CACHE[cache_key] = model
                logger.info(f"Loaded and cached {model_type} model")

            elif model_type == "reranker":
                import torch
                from transformers import AutoTokenizer, AutoModelForSequenceClassification
                tokenizer = AutoTokenizer.from_pretrained(model_name)
                model = AutoModelForSequenceClassification.from_pretrained(model_name)
                model.eval()
                _GLOBAL_MODEL_CACHE[f"{cache_key}_tokenizer"] = tokenizer
                _GLOBAL_MODEL_CACHE[f"{cache_key}_model"] = model
                logger.info(f"Loaded and cached reranker model")

        except Exception as e:
            logger.error(f"Failed to load {model_type} model: {e}")
            _GLOBAL_MODEL_CACHE[cache_key] = None

    return _GLOBAL_MODEL_CACHE.get(cache_key)


class RetrievalConfig(BaseModel):
    """æ£€ç´¢é…ç½®"""
    # å…³é”®è¯åŒ¹é…é…ç½®
    enable_keyword_mapping: bool = True

    # æŸ¥è¯¢ç†è§£é…ç½®
    enable_query_understanding: bool = True
    enable_query_expansion: bool = True
    enable_intent_classification: bool = True
    enable_entity_extraction: bool = True

    # æ£€ç´¢ç­–ç•¥é…ç½®
    enable_sparse_retrieval: bool = True
    enable_dense_retrieval: bool = False
    enable_reranking: bool = False

    # å¤±è´¥å›é€€ç­–ç•¥
    fallback_strategy: str = "full_content"

    rerank_model_name: str = "cross-encoder/ms-marco-MiniLM-L-12-v2"

    sparse_weight: float = 1.0
    dense_weight: float = 0.0
    rerank_weight: float = 0.0

    top_k_candidates: int = 10
    final_top_k: int = 5

    # ===== æ–°å¢ï¼šç›¸ä¼¼åº¦é˜ˆå€¼é…ç½® =====
    sparse_min_score: float = 1.0  # BM25æœ€ä½åˆ†æ•°é˜ˆå€¼
    dense_min_score: float = 0.5  # å¯†é›†æ£€ç´¢æœ€ä½ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆä»0.3æé«˜åˆ°0.5ï¼‰
    keyword_min_relevance: float = 0.5  # å…³é”®è¯ç›¸å…³æ€§æœ€ä½é˜ˆå€¼

    # æ€§èƒ½ä¼˜åŒ–é…ç½®
    enable_async: bool = True
    max_content_length: int = 1000
    enable_caching: bool = True


class QueryUnderstanding:
    """æŸ¥è¯¢ç†è§£å±‚ - è½»é‡åŒ–ç‰ˆæœ¬"""

    def __init__(self, config: RetrievalConfig):
        self.config = config
        # ç®€åŒ–çš„æ„å›¾å…³é”®è¯
        self.intent_keywords = {
            "æ“ä½œæŒ‡å—": ["å¦‚ä½•", "æ€ä¹ˆ", "æ–¹æ³•", "æ­¥éª¤"],
            "æ—¶æœºåˆ¤æ–­": ["ä»€ä¹ˆæ—¶å€™", "ä½•æ—¶", "æ—¶æœº", "æ—¶é—´"],
            "å¯è¡Œæ€§": ["è¦ä¸è¦", "æ˜¯å¦", "é€‚åˆ", "å¯ä»¥", "èƒ½å¦", "åº”è¯¥"]
        }

        # ç®€åŒ–çš„åŒä¹‰è¯
        self.domain_synonyms = {
            "å‡ºæµ·": ["æµ·å¤–", "å›½é™…", "å…¨çƒ", "è·¨å¢ƒ"],
            "ä¼ä¸š": ["å…¬å¸", "é›†å›¢", "ç»„ç»‡"],
            "é£é™©": ["å±é™©", "å¨èƒ", "æŒ‘æˆ˜", "é—®é¢˜"]
        }

    async def process_query(self, query: str) -> Dict[str, Any]:
        """å¿«é€ŸæŸ¥è¯¢å¤„ç†"""
        start_time = time.time()

        result = {
            "original": query,
            "cleaned": self._clean_query(query),
            "intent": "è¯¾ç¨‹åŸ¹è®­æé—®",
            "entities": [],
            "expanded_query": query,
            "keywords": []
        }

        if self.config.enable_intent_classification:
            result["intent"] = self._classify_intent(query)

        if self.config.enable_query_expansion:
            result["expanded_query"] = self._expand_query(query)

        result["keywords"] = self._extract_keywords(query)

        process_time = time.time() - start_time
        logger.info(f"Query understanding completed in {process_time:.3f}s")
        return result

    def _clean_query(self, query: str) -> str:
        """å¿«é€Ÿæ¸…ç†æŸ¥è¯¢"""
        return re.sub(r'[^\w\s\u4e00-\u9fff]', '', query).strip()

    def _classify_intent(self, query: str) -> str:
        """å¿«é€Ÿæ„å›¾åˆ†ç±»"""
        query_lower = query.lower()
        max_score = 0
        best_intent = "è¯¾ç¨‹åŸ¹è®­æé—®"

        for intent, keywords in self.intent_keywords.items():
            score = sum(1 for keyword in keywords if keyword in query_lower)
            if score > max_score:
                max_score = score
                best_intent = intent

        return best_intent

    def _expand_query(self, query: str) -> str:
        """è½»é‡çº§æŸ¥è¯¢æ‰©å±•"""
        words = jieba.lcut(query)
        expanded_terms = list(words)  # å¤åˆ¶åŸè¯

        # åªæ·»åŠ æœ€é‡è¦çš„åŒä¹‰è¯
        for word in words[:3]:  # åªå¤„ç†å‰3ä¸ªè¯
            if word in self.domain_synonyms:
                expanded_terms.extend(self.domain_synonyms[word][:1])  # åªåŠ 1ä¸ªåŒä¹‰è¯

        return " ".join(expanded_terms)

    def _extract_keywords(self, query: str) -> List[str]:
        """å¿«é€Ÿå…³é”®è¯æå–"""
        return jieba.analyse.extract_tags(query, topK=3, withWeight=False)


class OptimizedSparseRetriever:
    """ä¼˜åŒ–çš„ç¨€ç–æ£€ç´¢å™¨"""

    def __init__(self, config: RetrievalConfig):
        self.config = config
        self.documents = []
        self.doc_word_counts = []  # é¢„è®¡ç®—è¯é¢‘
        self.doc_lens = []
        self.avgdl = 0
        self.idf = {}
        self.indexed = False

    def build_index(self, documents: List[Dict[str, Any]]):
        """æ„å»ºä¼˜åŒ–çš„ç´¢å¼•"""
        start_time = time.time()

        self.documents = documents
        self.doc_word_counts = []
        self.doc_lens = []

        # æ”¶é›†æ‰€æœ‰è¯æ±‡
        all_words = set()
        doc_words_list = []

        for doc in documents:
            content = doc.get('content', '') + ' ' + doc.get('title', '')
            # é™åˆ¶å†…å®¹é•¿åº¦
            if len(content) > self.config.max_content_length:
                content = content[:self.config.max_content_length]

            words = jieba.lcut(content.lower())
            doc_words_list.append(words)

            word_counts = Counter(words)
            self.doc_word_counts.append(word_counts)
            self.doc_lens.append(len(words))
            all_words.update(words)

        self.avgdl = sum(self.doc_lens) / len(self.doc_lens) if self.doc_lens else 0

        # è®¡ç®—IDF
        doc_freq = {}
        for words in doc_words_list:
            unique_words = set(words)
            for word in unique_words:
                doc_freq[word] = doc_freq.get(word, 0) + 1

        N = len(documents)
        self.idf = {}
        for word in all_words:
            df = doc_freq.get(word, 0)
            self.idf[word] = math.log((N - df + 0.5) / (df + 0.5))

        self.indexed = True
        index_time = time.time() - start_time
        logger.info(f"Sparse index built in {index_time:.3f}s for {len(documents)} docs")

    def search(self, processed_query: Dict[str, Any], top_k: int = 10) -> List[Dict[str, Any]]:
        """ä¼˜åŒ–çš„BM25æœç´¢"""
        if not self.indexed:
            return []

        start_time = time.time()

        query_words = jieba.lcut(processed_query['expanded_query'].lower())
        scores = []

        k1, b = 1.5, 0.75  # BM25å‚æ•°

        for i, doc in enumerate(self.documents):
            score = 0
            doc_word_counts = self.doc_word_counts[i]

            for word in query_words:
                if word in doc_word_counts and word in self.idf:
                    tf = doc_word_counts[word]
                    idf = self.idf[word]

                    # BM25å…¬å¼
                    numerator = tf * (k1 + 1)
                    denominator = tf + k1 * (1 - b + b * self.doc_lens[i] / self.avgdl)
                    score += idf * (numerator / denominator)

            # æ ‡é¢˜é¢å¤–åŠ æƒ
            title = self.documents[i].get('title', '').lower()
            title_matches = sum(1 for word in query_words if word in title)
            score += title_matches * 2.0

            if score >= self.config.sparse_min_score:  # åªä¿ç•™æœ‰åˆ†æ•°çš„ç»“æœ
                scores.append({
                    'doc_id': i,
                    'score': score,
                    'content': self.documents[i].get('content', ''),
                    'title': self.documents[i].get('title', ''),
                    'metadata': self.documents[i].get('metadata', {})
                })

        # æŒ‰åˆ†æ•°æ’åº
        scores.sort(key=lambda x: x['score'], reverse=True)

        search_time = time.time() - start_time
        logger.info(f"Sparse search completed in {search_time:.3f}s, found {len(scores)} results")

        return scores[:top_k]


class OptimizedDenseRetriever:
    """ä¼˜åŒ–çš„å¯†é›†æ£€ç´¢å™¨ - å¯é€‰ä½¿ç”¨"""

    def __init__(self, config: RetrievalConfig):
        self.config = config
        self.documents = []
        self.document_embeddings = None
        self.model = None

    def _get_model(self):
        """è·å–ç¼“å­˜çš„æ¨¡å‹"""
        if self.model is None:
            self.model = get_cached_model("sentence_transformer",
                                          'paraphrase-multilingual-MiniLM-L12-v2')
        return self.model

    def build_index(self, documents: List[Dict[str, Any]]):
        """é¢„è®¡ç®—æ–‡æ¡£å‘é‡"""
        if not self.config.enable_dense_retrieval:
            return

        start_time = time.time()
        model = self._get_model()
        if model is None:
            logger.warning("Dense retrieval model not available")
            return

        self.documents = documents

        # å‡†å¤‡æ–‡æ¡£æ–‡æœ¬
        doc_texts = []
        for doc in documents:
            text = doc.get('title', '') + ' ' + doc.get('content', '')
            # é™åˆ¶é•¿åº¦
            if len(text) > self.config.max_content_length:
                text = text[:self.config.max_content_length]
            doc_texts.append(text)

        try:
            # é¢„è®¡ç®—æ‰€æœ‰æ–‡æ¡£çš„å‘é‡
            self.document_embeddings = model.encode(doc_texts, show_progress_bar=False)
            build_time = time.time() - start_time
            logger.info(f"Dense index built in {build_time:.3f}s")
        except Exception as e:
            logger.error(f"Failed to build dense index: {e}")
            self.document_embeddings = None

    def search(self, processed_query: Dict[str, Any], top_k: int = 10) -> List[Dict[str, Any]]:
        """å¿«é€Ÿå‘é‡æœç´¢"""
        if not self.config.enable_dense_retrieval or self.document_embeddings is None:
            return []

        start_time = time.time()
        model = self._get_model()
        if model is None:
            return []

        try:
            # è®¡ç®—æŸ¥è¯¢å‘é‡
            query_embedding = model.encode([processed_query['expanded_query']])

            # è®¡ç®—ç›¸ä¼¼åº¦
            from sklearn.metrics.pairwise import cosine_similarity
            similarities = cosine_similarity(query_embedding, self.document_embeddings)[0]

            # æ„å»ºç»“æœ
            results = []
            for i, sim in enumerate(similarities):
                if sim >= self.config.dense_min_score:  # è¿‡æ»¤ä½ç›¸ä¼¼åº¦ç»“æœ ï¼Œç›¸ä¼¼åº¦è®¾ç½®è¾ƒä½å¾—åˆ°é«˜å¬å›
                    results.append({
                        'doc_id': i,
                        'score': float(sim),
                        'content': self.documents[i].get('content', ''),
                        'title': self.documents[i].get('title', ''),
                        'metadata': self.documents[i].get('metadata', {})
                    })

            results.sort(key=lambda x: x['score'], reverse=True)

            search_time = time.time() - start_time
            logger.info(f"Dense search completed in {search_time:.3f}s")
            logger.info(
                f"Dense search completed in {search_time:.3f}s, found {len(results)} results above threshold {self.config.dense_min_score}")
            return results[:top_k]

        except Exception as e:
            logger.error(f"Dense search failed: {e}")
            return []


class FastKnowledgeRetriever:
    def __init__(self, config: RetrievalConfig = None):
        self.config = config or RetrievalConfig()
        self.query_processor = QueryUnderstanding(self.config)
        self.sparse_retriever = OptimizedSparseRetriever(self.config)
        self.dense_retriever = OptimizedDenseRetriever(self.config) if self.config.enable_dense_retrieval else None
        self.is_indexed = False
        self.course_config = None
        self.documents = []

    def _is_meaningless_input(self, query: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæ— æ„ä¹‰è¾“å…¥"""
        # æ£€æŸ¥æ˜¯å¦åªåŒ…å«é‡å¤å­—ç¬¦æˆ–æ— æ„ä¹‰å­—ç¬¦
        meaningless_patterns = [
            r'^[å“ˆå˜¿å‘µå˜»å—¯å“¦å•Šå‘€å“Ÿé¢å—¯]+$',  # ç¬‘å£°æˆ–è¯­æ°”è¯
            r'^[0-9]+$',  # çº¯æ•°å­—
            r'^[a-zA-Z]{1,3}$',  # å•ä¸ªæˆ–å°‘æ•°å­—æ¯
            r'^[\s\W]*$',  # åªæœ‰ç©ºæ ¼æˆ–ç‰¹æ®Šå­—ç¬¦
            r'^(.)\1{2,}$',  # åŒä¸€å­—ç¬¦é‡å¤3æ¬¡ä»¥ä¸Š
        ]

        for pattern in meaningless_patterns:
            if re.match(pattern, query):
                return True

        # æ£€æŸ¥æŸ¥è¯¢é•¿åº¦
        if len(query) < 2:
            return True

        return False

    async def retrieve(self, query: str, top_k: int = None) -> Dict[str, Any]:
        """ä¸»æ£€ç´¢æ–¹æ³• - ä¼˜åŒ–ç‰ˆæœ¬ï¼šç§»é™¤äº†å»¶è¿Ÿæ„å»ºé€»è¾‘"""
        total_start_time = time.time()

        if not self.is_indexed:
            return {
                "matched": False,
                "topic": "",
                "content": "",
                "score": 0.0,
                "message": "Index not built"
            }

        # ç¬¬ä¸€å±‚ï¼šå…³é”®è¯æ˜ å°„å¿«é€Ÿæ£€ç´¢
        if self.config.enable_keyword_mapping:
            keyword_result = self.keyword_mapping_search(query)
            if keyword_result and keyword_result["matched"]:
                total_time = time.time() - total_start_time
                keyword_result["total_time"] = round(total_time, 4)
                logger.info(f"Keyword mapping success. Total time: {total_time:.4f}s")
                return keyword_result

        # ç¬¬äºŒå±‚ï¼šå¤±è´¥å›é€€ç­–ç•¥
        if self.config.fallback_strategy == "full_content":
            # è¿”å›å®Œæ•´åŸ¹è®­å†…å®¹
            total_time = time.time() - total_start_time
            full_content = self.course_config.get("full_training_content", "") if self.course_config else ""
            logger.info(f"Using full content fallback in {total_time:.4f}s")
            return {
                "matched": False,
                "topic": "",
                "content": full_content,
                "score": 0.0,
                "message": "Using full training content - no keyword match found",
                "retrieval_time": round(total_time, 4)
            }

        elif self.config.fallback_strategy == "advanced_retrieval":
            # ç›´æ¥ä½¿ç”¨å·²æ„å»ºçš„ç´¢å¼•è¿›è¡Œé«˜çº§æ£€ç´¢
            return await self.advanced_retrieval(query, top_k)

        else:
            # æœªçŸ¥ç­–ç•¥
            total_time = time.time() - total_start_time
            return {
                "matched": False,
                "topic": "",
                "content": "",
                "score": 0.0,
                "message": f"Unknown fallback strategy: {self.config.fallback_strategy}",
                "retrieval_time": round(total_time, 4)
            }

    def keyword_mapping_search(self, query: str) -> Optional[Dict[str, Any]]:
        """çº¯å­—ç¬¦ä¸²åŒ¹é…ï¼Œå®Œå…¨ä¸ä½¿ç”¨jieba"""
        if not self.course_config:
            return None

        topic_mapping = self.course_config.get("topic_mapping", {})
        structured_knowledge = self.course_config.get("structured_knowledge", {})

        if not topic_mapping or not structured_knowledge:
            return None

        start_time = time.time()

        # çº¯å­—ç¬¦ä¸²å¤„ç†
        query_lower = query.lower().strip()
        query_clean = re.sub(r'[å•Šå“¦å‘¢å§å‘€ï¼Ÿï¼ã€‚ï¼Œã€ï¼›ï¼š"""''ï¼ˆï¼‰ã€ã€‘\s]', '', query_lower)
        if self._is_meaningless_input(query_clean):
            logger.info(f"Detected meaningless input: '{query}', skipping keyword mapping")
            return None
        # 1. å®Œå…¨åŒ¹é…
        for key, mapped_topic in topic_mapping.items():
            key_lower = key.lower()
            key_clean = re.sub(r'[å•Šå“¦å‘¢å§å‘€ï¼Ÿï¼ã€‚ï¼Œã€ï¼›ï¼š"""''ï¼ˆï¼‰ã€ã€‘\s]', '', key_lower)
            if key_clean == query_clean or key_lower == query_lower:
                if mapped_topic in structured_knowledge:
                    retrieval_time = time.time() - start_time
                    logger.info(f"Exact keyword mapping: {key} -> {mapped_topic} in {retrieval_time:.4f}s")
                    return self._create_match_result(mapped_topic, structured_knowledge[mapped_topic], 10.0,
                                                     "Exact keyword mapping", retrieval_time)

        # 2. åŒ…å«åŒ¹é…
        for key, mapped_topic in topic_mapping.items():
            key_lower = key.lower()
            key_clean = re.sub(r'[å•Šå“¦å‘¢å§å‘€ï¼Ÿï¼ã€‚ï¼Œã€ï¼›ï¼š"""''ï¼ˆï¼‰ã€ã€‘\s]', '', key_lower)
            if key_clean in query_clean or key_lower in query_lower or query_clean in key_clean:
                if mapped_topic in structured_knowledge:
                    retrieval_time = time.time() - start_time
                    logger.info(f"Partial keyword mapping: {key} -> {mapped_topic} in {retrieval_time:.4f}s")
                    return self._create_match_result(mapped_topic, structured_knowledge[mapped_topic], 8.0,
                                                     "Partial keyword mapping", retrieval_time)

        retrieval_time = time.time() - start_time
        logger.info(f"No keyword mapping found in {retrieval_time:.4f}s")
        return None

    def _create_match_result(self, topic: str, content: str, score: float,
                             message: str, retrieval_time: float) -> Dict[str, Any]:
        """åˆ›å»ºåŒ¹é…ç»“æœ"""
        return {
            "matched": True,
            "topic": topic,
            "content": content.strip(),
            "score": score,
            "message": message,
            "retrieval_time": round(retrieval_time, 4)
        }

    async def advanced_retrieval(self, query: str, top_k: int = None) -> Dict[str, Any]:
        """é«˜çº§æ£€ç´¢ - ä½¿ç”¨é¢„æ„å»ºçš„ç´¢å¼•"""
        if not self.is_indexed:
            return {
                "matched": False,
                "topic": "",
                "content": "",
                "score": 0.0,
                "message": "Index not built for advanced retrieval"
            }

        start_time = time.time()
        top_k = top_k or self.config.final_top_k

        # 1. æŸ¥è¯¢ç†è§£
        processed_query = await self.query_processor.process_query(query)

        # 2. å¤šè·¯æ£€ç´¢
        all_candidates = []

        if self.config.enable_sparse_retrieval:
            sparse_results = self.sparse_retriever.search(processed_query, self.config.top_k_candidates)
            for result in sparse_results:
                result['source'] = 'sparse'
            all_candidates.extend(sparse_results)

        if self.config.enable_dense_retrieval and self.dense_retriever:
            dense_results = self.dense_retriever.search(processed_query, self.config.top_k_candidates)
            for result in dense_results:
                result['source'] = 'dense'
            all_candidates.extend(dense_results)

        # 3. ç»“æœæ’åºå’Œè¿”å›
        if all_candidates:
            final_results = sorted(all_candidates, key=lambda x: x['score'], reverse=True)[:top_k]
            best_result = final_results[0]
            min_score_threshold = self.config.dense_min_score if best_result.get(
                'source') == 'dense' else self.config.sparse_min_score
            total_time = time.time() - start_time
            if best_result['score'] < min_score_threshold:
                logger.info(f"Best result score {best_result['score']:.4f} below threshold {min_score_threshold}")
                return {
                    "matched": False,
                    "topic": "",
                    "content": "",
                    "score": round(best_result.get('score', 0), 4),
                    "message": f"No results above similarity threshold (score: {round(best_result['score'], 4)}, threshold: {min_score_threshold})",
                    "retrieval_time": round(total_time, 3)
                }
            logger.info(f"Advanced retrieval completed in {total_time:.3f}s")

            return {
                "matched": True,
                "topic": best_result.get('title', ''),
                "content": best_result.get('content', ''),
                "score": round(best_result.get('score', 0), 4),
                "message": f"Found via {best_result.get('source', 'unknown')} retrieval",
                "intent": processed_query.get('intent', ''),
                "entities": processed_query.get('entities', []),
                "retrieval_time": round(total_time, 3)
            }
        else:
            total_time = time.time() - start_time
            return {
                "matched": False,
                "topic": "",
                "content": "",
                "score": 0.0,
                "message": "No relevant results found in advanced retrieval",
                "retrieval_time": round(total_time, 3)
            }


# ===== æ–°å¢ï¼šæ–°é—»æ£€ç´¢åŠŸèƒ½ =====

def extract_news_keywords(user_input: str) -> str:
    """
    æ™ºèƒ½æå–æ–°é—»æœç´¢å…³é”®è¯

    Args:
        user_input (str): ç”¨æˆ·è¾“å…¥çš„åŸå§‹é—®é¢˜

    Returns:
        str: æå–å‡ºçš„å…³é”®è¯
    """
    logger.info(f"å¼€å§‹æå–æ–°é—»å…³é”®è¯ï¼ŒåŸå§‹è¾“å…¥: '{user_input}'")

    # å»é™¤å¸¸è§çš„ç–‘é—®è¯å’Œè¯­æ°”è¯
    stop_words = [
        'å—', 'å‘¢', 'äº†', 'çš„', 'åœ¨', 'æ˜¯', 'æœ‰', 'æ²¡æœ‰', 'ä¼š', 'èƒ½', 'å¯ä»¥', 'åº”è¯¥',
        'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'å“ªé‡Œ', 'å“ªä¸ª', 'å¤šå°‘', 'å‡ ', 'ï¼Ÿ', '?', 'ï¼', '!',
        'ã€', 'ï¼Œ', ',', 'ã€‚', '.', 'ï¼š', ':', 'ï¼›', ';'
    ]

    # å»é™¤ç–‘é—®å¥å¼
    question_patterns = [
        r'.*äº†å—\ï¼Ÿ?$',
        r'.*å‘¢\ï¼Ÿ?$',
        r'.*å—\ï¼Ÿ?$',
        r'^æ˜¯å¦.*',
        r'^æœ‰æ²¡æœ‰.*',
        r'^ä¼šä¸ä¼š.*',
        r'^èƒ½ä¸èƒ½.*'
    ]

    cleaned_input = user_input.strip()

    # å¤„ç†ç–‘é—®å¥å¼
    for pattern in question_patterns:
        if re.match(pattern, cleaned_input):
            # ç§»é™¤ç–‘é—®å¥å°¾
            cleaned_input = re.sub(r'äº†å—\ï¼Ÿ?$|å‘¢\ï¼Ÿ?$|å—\ï¼Ÿ?$', '', cleaned_input)
            cleaned_input = re.sub(r'^æ˜¯å¦|^æœ‰æ²¡æœ‰|^ä¼šä¸ä¼š|^èƒ½ä¸èƒ½', '', cleaned_input)
            break

    # ä½¿ç”¨jiebaè¿›è¡Œåˆ†è¯å’Œå…³é”®è¯æå–
    words = jieba.lcut(cleaned_input)

    # è¿‡æ»¤åœç”¨è¯
    filtered_words = []
    for word in words:
        if word not in stop_words and len(word.strip()) > 0:
            filtered_words.append(word)

    # é‡æ–°ç»„åˆæˆå…³é”®è¯
    if filtered_words:
        keyword = ''.join(filtered_words)
    else:
        # å¦‚æœè¿‡æ»¤åæ²¡æœ‰è¯ï¼Œä½¿ç”¨åŸå§‹è¾“å…¥å»é™¤æ ‡ç‚¹ç¬¦å·
        keyword = re.sub(r'[ï¼Ÿï¼Ÿï¼ï¼ã€‚ï¼Œã€ï¼›ï¼š\s]+', '', user_input)

    logger.info(f"å…³é”®è¯æå–å®Œæˆ: '{user_input}' -> '{keyword}'")
    logger.info(f"åˆ†è¯ç»“æœ: {words}")
    logger.info(f"è¿‡æ»¤åçš„è¯: {filtered_words}")

    return keyword


async def search_news_articles(keyword: str, articles_count: int = 3, days_back: int = 31) -> Dict[str, Any]:
    """
    æœç´¢ç›¸å…³æ–°é—»æ–‡ç« 

    Parameters:
        keyword (str): æœç´¢å…³é”®è¯
        articles_count (int): è¿”å›æ–‡ç« æ•°é‡ï¼Œé»˜è®¤3ç¯‡
        days_back (int): æœç´¢æœ€è¿‘å¤šå°‘å¤©çš„æ–°é—»ï¼Œé»˜è®¤31å¤©

    Returns:
        Dict[str, Any]: åŒ…å«æœç´¢ç»“æœçš„å­—å…¸
    """
    logger.info(f"å¼€å§‹æœç´¢æ–°é—» - å…³é”®è¯: '{keyword}', æ•°é‡: {articles_count}, å¤©æ•°: {days_back}")

    try:
        # æ„å»ºè¯·æ±‚å‚æ•°
        request_params = {
            "action": "getArticles",
            "keyword": keyword,
            "sourceLocationUri": [
                "http://en.wikipedia.org/wiki/China",
                "http://en.wikipedia.org/wiki/United_States",
                "http://en.wikipedia.org/wiki/Canada",
                "http://en.wikipedia.org/wiki/United_Kingdom"
            ],
            "ignoreSourceGroupUri": "paywall/paywalled_sources",
            "articlesPage": 1,
            "articlesCount": min(articles_count, 10),  # é™åˆ¶æœ€å¤š10ç¯‡
            "articlesSortBy": "date",
            "articlesSortByAsc": False,  # æœ€æ–°çš„åœ¨å‰
            "dataType": ["news", "pr"],
            "forceMaxDataTimeWindow": days_back,
            "resultType": "articles",
            "apiKey": NEWS_API_KEY
        }

        logger.info(f"æ–°é—»APIè¯·æ±‚å‚æ•°: {json.dumps(request_params, indent=2, ensure_ascii=False)}")

        # å‘èµ·å¼‚æ­¥HTTPè¯·æ±‚
        async with aiohttp.ClientSession() as session:
            logger.info(f"å‘èµ·HTTPè¯·æ±‚åˆ°: {NEWS_API_URL}")
            async with session.post(NEWS_API_URL, json=request_params, timeout=30) as response:
                logger.info(f"HTTPå“åº”çŠ¶æ€ç : {response.status}")

                if response.status == 200:
                    data = await response.json()
                    logger.info(f"APIè¿”å›æ•°æ®ç»“æ„: {list(data.keys()) if isinstance(data, dict) else 'Not a dict'}")

                    # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡ç« ç»“æœ
                    if "articles" in data and "results" in data["articles"]:
                        articles = data["articles"]["results"]
                        total_results = data["articles"].get("totalResults", 0)

                        logger.info(f"æ‰¾åˆ°æ–‡ç« æ•°é‡: {len(articles)}, æ€»ç»“æœæ•°: {total_results}")

                        if not articles:
                            logger.warning(f"æœªæ‰¾åˆ°å…³äº'{keyword}'çš„ç›¸å…³æ–°é—»")
                            return {
                                "success": False,
                                "message": f"æœªæ‰¾åˆ°å…³äº'{keyword}'çš„ç›¸å…³æ–°é—»",
                                "articles": []
                            }

                        # å¤„ç†æ–‡ç« æ•°æ®ï¼Œæå–å…³é”®ä¿¡æ¯
                        processed_articles = []
                        for i, article in enumerate(articles[:articles_count]):
                            logger.info(f"å¤„ç†ç¬¬{i + 1}ç¯‡æ–‡ç« : {article.get('title', 'æ— æ ‡é¢˜')[:50]}...")

                            processed_article = {
                                "title": article.get("title", "æ— æ ‡é¢˜"),
                                "date": article.get("date", "æœªçŸ¥æ—¥æœŸ"),
                                "time": article.get("time", "æœªçŸ¥æ—¶é—´"),
                                "url": article.get("url", ""),
                                "source": article.get("source", {}).get("title", "æœªçŸ¥æ¥æº"),
                                "body": article.get("body", "æ— å†…å®¹")[:1000] + "..." if len(
                                    article.get("body", "")) > 1000 else article.get("body", "æ— å†…å®¹"),  # é™åˆ¶æ­£æ–‡é•¿åº¦
                                "language": article.get("lang", "æœªçŸ¥"),
                                "relevance": article.get("relevance", 0)
                            }
                            processed_articles.append(processed_article)

                        logger.info(f"æˆåŠŸå¤„ç†{len(processed_articles)}ç¯‡æ–‡ç« ")

                        return {
                            "success": True,
                            "message": f"æ‰¾åˆ° {len(processed_articles)} ç¯‡å…³äº'{keyword}'çš„ç›¸å…³æ–°é—»",
                            "keyword": keyword,
                            "total_results": total_results,
                            "articles": processed_articles
                        }
                    else:
                        logger.error(f"APIè¿”å›æ•°æ®æ ¼å¼å¼‚å¸¸ - æ•°æ®ç»“æ„: {data}")
                        return {
                            "success": False,
                            "message": f"APIè¿”å›æ•°æ®æ ¼å¼å¼‚å¸¸ï¼Œæœªæ‰¾åˆ°å…³äº'{keyword}'çš„æ–°é—»",
                            "articles": []
                        }
                else:
                    logger.error(f"æ–°é—»APIè¯·æ±‚å¤±è´¥ - çŠ¶æ€ç : {response.status}, å“åº”å†…å®¹: {await response.text()}")
                    return {
                        "success": False,
                        "message": f"æ–°é—»APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}",
                        "articles": []
                    }

    except asyncio.TimeoutError:
        logger.error("æ–°é—»APIè¯·æ±‚è¶…æ—¶")
        return {
            "success": False,
            "message": "æ–°é—»APIè¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•",
            "articles": []
        }
    except Exception as e:
        logger.error(f"æœç´¢æ–°é—»æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)
        return {
            "success": False,
            "message": f"æœç´¢æ–°é—»æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
            "articles": []
        }


def summarize_news_articles(news_result: Dict[str, Any]) -> str:
    """
    æ€»ç»“æ–°é—»æ–‡ç« å†…å®¹

    Parameters:
        news_result (Dict[str, Any]): æ–°é—»æœç´¢ç»“æœ

    Returns:
        str: æ–°é—»æ€»ç»“
    """
    logger.info("å¼€å§‹ç”Ÿæˆæ–°é—»æ€»ç»“")

    if not news_result.get("success") or not news_result.get("articles"):
        error_message = f"æ–°é—»æœç´¢å¤±è´¥: {news_result.get('message', 'æœªçŸ¥é”™è¯¯')}"
        logger.info(f"æ–°é—»æ€»ç»“ç»“æœ: {error_message}")
        return error_message

    articles = news_result["articles"]
    keyword = news_result.get("keyword", "ç›¸å…³ä¸»é¢˜")

    summary_parts = [
        f"ğŸ” å…³äº'{keyword}'çš„æœ€æ–°æ–°é—»æ€»ç»“:",
        f"ğŸ“Š å…±æ‰¾åˆ° {news_result.get('total_results', 0)} ç¯‡ç›¸å…³æŠ¥é“ï¼Œä»¥ä¸‹æ˜¯æœ€æ–°çš„ {len(articles)} ç¯‡:",
        ""
    ]

    for i, article in enumerate(articles, 1):
        summary_parts.extend([
            f"ğŸ“° æ–°é—» {i}:",
            f"   æ ‡é¢˜: {article['title']}",
            f"   æ—¶é—´: {article['date']} {article['time']}",
            f"   æ¥æº: {article['source']}",
            f"   è¯­è¨€: {article['language']}",
            f"   æ‘˜è¦: {article['body'][:200]}{'...' if len(article['body']) > 200 else ''}",
            f"   é“¾æ¥: {article['url']}",
            ""
        ])

    summary_parts.append("ğŸ’¡ è¿™äº›æ˜¯æ‚¨è¯¢é—®çš„æ–°é—»å†…å®¹å—ï¼Ÿå¦‚æœéœ€è¦äº†è§£å…·ä½“æŸç¯‡æ–°é—»çš„è¯¦ç»†å†…å®¹ï¼Œè¯·å‘Šè¯‰æˆ‘ã€‚")

    summary = "\n".join(summary_parts)
    logger.info(f"æ–°é—»æ€»ç»“ç”Ÿæˆå®Œæˆï¼Œæ€»é•¿åº¦: {len(summary)} å­—ç¬¦")

    return summary


# æ–°å¢ï¼šæ–°é—»æœç´¢å‡½æ•°ï¼ˆfunction callå…¥å£ï¼‰
async def get_latest_news(keyword: str, count: int = 3, days: int = 31) -> Dict[str, Any]:
    """æœç´¢æœ€æ–°ç›¸å…³æ–°é—»

    Parameters:
        keyword (str): æœç´¢å…³é”®è¯
        count (int): è¿”å›æ–°é—»æ•°é‡ï¼Œé»˜è®¤3ç¯‡ï¼Œæœ€å¤š10ç¯‡
        days (int): æœç´¢æœ€è¿‘å¤šå°‘å¤©çš„æ–°é—»ï¼Œé»˜è®¤31å¤©

    Returns:
        Dict[str, Any]: åŒ…å«æ–°é—»æœç´¢ç»“æœå’Œæ€»ç»“çš„å­—å…¸
    """
    logger.info(f"=== å¼€å§‹æ–°é—»æœç´¢å‡½æ•°è°ƒç”¨ ===")
    logger.info(f"å…¥å‚ - å…³é”®è¯: '{keyword}', æ•°é‡: {count}, å¤©æ•°: {days}")

    try:
        # é™åˆ¶å‚æ•°èŒƒå›´
        count = min(max(count, 1), 10)  # 1-10ç¯‡
        days = min(max(days, 1), 90)  # 1-90å¤©

        logger.info(f"å‚æ•°èŒƒå›´é™åˆ¶å - æ•°é‡: {count}, å¤©æ•°: {days}")

        # æœç´¢æ–°é—»
        logger.info("å¼€å§‹è°ƒç”¨ search_news_articles")
        news_result = await search_news_articles(keyword, count, days)
        logger.info(
            f"search_news_articles è¿”å›ç»“æœ: success={news_result.get('success')}, æ–‡ç« æ•°={len(news_result.get('articles', []))}")

        # ç”Ÿæˆæ€»ç»“
        logger.info("å¼€å§‹ç”Ÿæˆæ–°é—»æ€»ç»“")
        summary = summarize_news_articles(news_result)
        logger.info("æ–°é—»æ€»ç»“ç”Ÿæˆå®Œæˆ")

        final_result = {
            "success": news_result["success"],
            "keyword": keyword,
            "search_params": {
                "count": count,
                "days": days
            },
            "raw_result": news_result,
            "summary": summary,
            "message": news_result.get("message", "")
        }

        logger.info(f"=== æ–°é—»æœç´¢å‡½æ•°è°ƒç”¨å®Œæˆ ===")
        logger.info(f"æœ€ç»ˆç»“æœ: success={final_result['success']}, å…³é”®è¯='{final_result['keyword']}'")

        return final_result

    except Exception as e:
        logger.error(f"æ–°é—»æœç´¢å‡½æ•°å‘ç”Ÿå¼‚å¸¸: {str(e)}", exc_info=True)
        error_result = {
            "success": False,
            "keyword": keyword,
            "error": str(e),
            "summary": f"æœç´¢å…³äº'{keyword}'çš„æ–°é—»æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
            "message": "æ–°é—»æœç´¢åŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨"
        }

        logger.info(f"=== æ–°é—»æœç´¢å‡½æ•°è°ƒç”¨å¼‚å¸¸ç»“æŸ ===")
        return error_result


# åˆ›å»ºå…¨å±€æœåŠ¡å®ä¾‹ï¼ˆå¯åŠ¨æ—¶æ„å»ºç´¢å¼•ï¼‰
_GLOBAL_SERVICE_INSTANCE = None


def get_service_instance():
    """è·å–å…¨å±€æœåŠ¡å®ä¾‹"""
    global _GLOBAL_SERVICE_INSTANCE
    if _GLOBAL_SERVICE_INSTANCE is None:
        logger.info("Creating global service instance...")
        _GLOBAL_SERVICE_INSTANCE = OptimizedRealtimeFunctionCallService()
    return _GLOBAL_SERVICE_INSTANCE


# åŸæœ‰æ¥å£é€‚é…
class FunctionCallQuery(BaseModel):
    user_input: str
    function_call_name: str


class RealtimeFunctionCallInfo(BaseModel):
    topic: str
    action: str
    query: Optional[FunctionCallQuery] = None


class OptimizedRealtimeFunctionCallService:
    def __init__(self):
        self.prompt_config = self.load_prompt_config()
        self.courses_config = self.prompt_config.get("azure_realtime_function_call", {})
        self.retrievers = {}  # ç¼“å­˜æ£€ç´¢å™¨
        # æ–°å¢ï¼šå¯åŠ¨æ—¶é¢„æ„å»ºæ‰€æœ‰è¯¾ç¨‹çš„ç´¢å¼•
        self._prebuild_all_indexes()

    def _prebuild_all_indexes(self):
        """é¢„æ„å»ºæ‰€æœ‰è¯¾ç¨‹çš„ç´¢å¼•"""
        logger.info("Starting to prebuild indexes for all courses...")
        start_time = time.time()

        for topic, course_config in self.courses_config.items():
            try:
                # ä¸ºæ¯ä¸ªä¸»é¢˜é¢„æ„å»ºä¸¤ç§ç­–ç•¥çš„ç´¢å¼•
                for strategy in ["full_content", "advanced_retrieval"]:
                    cache_key = f"{topic}_{strategy}"
                    if cache_key not in self.retrievers:
                        logger.info(f"Prebuilding index for {topic} with {strategy} strategy...")
                        retriever = self._create_and_build_retriever(topic, course_config, strategy)
                        self.retrievers[cache_key] = retriever

            except Exception as e:
                logger.error(f"Failed to prebuild index for {topic}: {e}")

        total_time = time.time() - start_time
        logger.info(f"Finished prebuilding indexes in {total_time:.2f}s")

    def _create_and_build_retriever(self, topic: str, course_config: Dict[str, Any],
                                    fallback_strategy: str) -> FastKnowledgeRetriever:
        """åˆ›å»ºå¹¶æ„å»ºæ£€ç´¢å™¨ï¼ˆåŒ…æ‹¬ç´¢å¼•ï¼‰"""
        if fallback_strategy == "full_content":
            # è½»é‡çº§é…ç½®ï¼Œåªéœ€è¦å…³é”®è¯æ˜ å°„
            config = RetrievalConfig(
                enable_keyword_mapping=True,
                enable_query_understanding=False,
                enable_sparse_retrieval=False,
                enable_dense_retrieval=False,
                enable_reranking=False,
                fallback_strategy=fallback_strategy,
                enable_caching=True
            )

            retriever = FastKnowledgeRetriever(config)
            retriever.course_config = course_config
            retriever.documents = []
            retriever.is_indexed = True

        else:  # advanced_retrieval
            # å®Œæ•´é…ç½®
            config = RetrievalConfig(
                enable_keyword_mapping=True,
                enable_query_understanding=True,
                enable_sparse_retrieval=True,
                enable_dense_retrieval=True,
                enable_reranking=False,  # å¯é€‰
                fallback_strategy=fallback_strategy,
                top_k_candidates=10,
                final_top_k=5,
                max_content_length=800,
                enable_caching=True
            )

            retriever = FastKnowledgeRetriever(config)
            retriever.course_config = course_config

            # ç«‹å³æ„å»ºæ–‡æ¡£å’Œç´¢å¼•
            documents = []
            structured_knowledge = course_config.get("structured_knowledge", {})
            for title, content in structured_knowledge.items():
                documents.append({
                    'title': title,
                    'content': content,
                    'metadata': {'topic': topic}
                })

            if documents:
                # æ„å»ºæ‰€æœ‰ç´¢å¼•
                retriever.documents = documents
                retriever.is_indexed = True

                # æ„å»ºç¨€ç–ç´¢å¼•
                if config.enable_sparse_retrieval:
                    retriever.sparse_retriever.build_index(documents)

                # æ„å»ºå¯†é›†ç´¢å¼•
                if config.enable_dense_retrieval:
                    retriever.dense_retriever.build_index(documents)

        return retriever

    def get_or_create_retriever(self, topic: str, course_config: Dict[str, Any],
                                fallback_strategy: str = "full_content") -> FastKnowledgeRetriever:
        """è·å–æˆ–åˆ›å»ºæ£€ç´¢å™¨ - ä¼˜åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨é¢„æ„å»ºçš„ç´¢å¼•"""
        cache_key = f"{topic}_{fallback_strategy}"

        # å¦‚æœå·²ç»æœ‰é¢„æ„å»ºçš„æ£€ç´¢å™¨ï¼Œç›´æ¥è¿”å›
        if cache_key in self.retrievers:
            logger.info(f"Using pre-built retriever for {topic} with {fallback_strategy}")
            return self.retrievers[cache_key]

        # å¦‚æœæ²¡æœ‰é¢„æ„å»ºï¼ˆä¾‹å¦‚æ–°å¢çš„ä¸»é¢˜ï¼‰ï¼Œåˆ™åˆ›å»ºå¹¶æ„å»º
        logger.warning(f"No pre-built retriever found for {topic}, building now...")
        retriever = self._create_and_build_retriever(topic, course_config, fallback_strategy)
        self.retrievers[cache_key] = retriever

        return retriever

    @staticmethod
    def load_prompt_config():
        try:
            with open('./utils/prompt.json', encoding='utf-8') as config_file:
                return json.load(config_file)
        except Exception as e:
            logger.error(f"Failed to load prompt config file: {e}")
            return {}

    def get_course_config(self, topic: str) -> Optional[Dict[str, Any]]:
        if topic in self.courses_config:
            return self.courses_config[topic]
        return None

    def start_session(self, topic: str) -> Dict[str, Any]:
        course_config = self.get_course_config(topic)
        if course_config:
            return {
                "instructions": course_config.get("instructions", ""),
                "tools": course_config.get("tools", [])
            }
        return {}

    async def get_function_call_result(self, query: FunctionCallQuery, topic: str,
                                       fallback_strategy: str = "full_content") -> Dict[str, Any]:
        """è·å–å‡½æ•°è°ƒç”¨ç»“æœ - ä¼˜åŒ–è¿”å›æ ¼å¼"""
        logger.info(f"=== å¼€å§‹å¤„ç†å‡½æ•°è°ƒç”¨ ===")
        logger.info(f"ä¸»é¢˜: {topic}")
        logger.info(f"å›é€€ç­–ç•¥: {fallback_strategy}")
        logger.info(f"å‡½æ•°å: {query.function_call_name}")
        logger.info(f"ç”¨æˆ·è¾“å…¥: '{query.user_input}'")

        course_config = self.get_course_config(topic)
        if not course_config:
            logger.error(f"Course config not found for topic '{topic}'")
            return {"result": "", "error": "Topic not found"}

        func_name = query.function_call_name
        user_input = query.user_input

        logger.info(f"Processing function call: {func_name} with input: {user_input}, fallback: {fallback_strategy}")

        # å¤„ç†æ–°é—»æœç´¢å‡½æ•°è°ƒç”¨
        if func_name == "get_latest_news":
            logger.info("è¯†åˆ«ä¸ºæ–°é—»æœç´¢å‡½æ•°è°ƒç”¨")
            try:
                # æ™ºèƒ½æå–å…³é”®è¯
                logger.info("å¼€å§‹æå–æœç´¢å…³é”®è¯")
                keyword = extract_news_keywords(user_input)
                logger.info(f"æå–çš„å…³é”®è¯: '{keyword}'")

                # è°ƒç”¨æ–°é—»æœç´¢å‡½æ•°
                logger.info("å¼€å§‹è°ƒç”¨æ–°é—»æœç´¢API")
                news_result = await get_latest_news(keyword, count=3, days=31)
                logger.info(f"æ–°é—»æœç´¢å®Œæˆ: success={news_result.get('success')}")

                # æ„å»ºç»Ÿä¸€æ ¼å¼çš„è¿”å›ç»“æœ
                if news_result.get("success"):
                    result_content = news_result.get("summary", "")
                else:
                    result_content = ""

                final_result = {
                    "result": result_content,
                    func_name: news_result
                }
                logger.info(f"=== æ–°é—»æœç´¢å‡½æ•°è°ƒç”¨å®Œæˆ ===")
                return final_result

            except Exception as e:
                logger.error(f"æ–°é—»æœç´¢å‡½æ•°è°ƒç”¨å¼‚å¸¸: {str(e)}", exc_info=True)
                error_result = {
                    "result": "",
                    func_name: {
                        "success": False,
                        "keyword": user_input,
                        "error": str(e),
                        "summary": f"æœç´¢æ–°é—»æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                        "message": "æ–°é—»æœç´¢åŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨"
                    }
                }
                logger.info(f"=== æ–°é—»æœç´¢å‡½æ•°è°ƒç”¨å¼‚å¸¸ç»“æŸ ===")
                return error_result

        # å¤„ç†åŸ¹è®­çŸ¥è¯†æ£€ç´¢å‡½æ•°è°ƒç”¨
        elif func_name in ["get_enterprise_going_global_info", "get_huiren_training_info"]:
            logger.info(f"è¯†åˆ«ä¸ºåŸ¹è®­çŸ¥è¯†æ£€ç´¢å‡½æ•°è°ƒç”¨: {func_name}")

            # è·å–é¢„æ„å»ºçš„æ£€ç´¢å™¨
            logger.info("è·å–é¢„æ„å»ºçš„æ£€ç´¢å™¨")
            retriever = self.get_or_create_retriever(topic, course_config, fallback_strategy)
            if not retriever:
                logger.error("æ£€ç´¢å™¨è·å–å¤±è´¥")
                return {"result": "", "error": "Retriever not available"}

            # æ‰§è¡Œæ£€ç´¢
            logger.info("å¼€å§‹æ‰§è¡ŒçŸ¥è¯†æ£€ç´¢")
            knowledge_result = await retriever.retrieve(user_input)
            logger.info(
                f"çŸ¥è¯†æ£€ç´¢å®Œæˆ: matched={knowledge_result.get('matched')}, topic='{knowledge_result.get('topic')}'")

            # æ„å»ºç»Ÿä¸€æ ¼å¼çš„è¿”å›ç»“æœ
            if knowledge_result.get("matched") or knowledge_result.get("content"):
                result_content = knowledge_result.get("content", "")
            else:
                result_content = ""

            final_result = {
                "result": result_content,
                func_name: knowledge_result
            }
            logger.info(f"=== åŸ¹è®­çŸ¥è¯†æ£€ç´¢å‡½æ•°è°ƒç”¨å®Œæˆ ===")
            return final_result

        else:
            logger.error(f"Unknown function call: {func_name}")
            logger.info(f"=== æœªçŸ¥å‡½æ•°è°ƒç”¨ç»“æŸ ===")
            return {"result": "", "error": f"Unknown function: {func_name}"}


async def realtime_function_call(fc_info: RealtimeFunctionCallInfo,
                                 fallback_strategy: str = "full_content") -> Dict[str, Any]:
    """å¤„ç†å®æ—¶å‡½æ•°è°ƒç”¨æ¥å£ - ä¼˜åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨å…¨å±€æœåŠ¡å®ä¾‹ï¼Œè¿”å›ç»Ÿä¸€æ ¼å¼"""
    logger.info("==================== å‡½æ•°è°ƒç”¨å¼€å§‹ ====================")
    logger.info(f"è¯·æ±‚ä¿¡æ¯:")
    logger.info(f"  - ä¸»é¢˜ (topic): {fc_info.topic}")
    logger.info(f"  - åŠ¨ä½œ (action): {fc_info.action}")
    logger.info(f"  - å›é€€ç­–ç•¥ (fallback_strategy): {fallback_strategy}")

    if fc_info.query:
        logger.info(f"  - æŸ¥è¯¢ä¿¡æ¯:")
        logger.info(f"    - ç”¨æˆ·è¾“å…¥: '{fc_info.query.user_input}'")
        logger.info(f"    - å‡½æ•°åç§°: {fc_info.query.function_call_name}")

    try:
        # ä½¿ç”¨å…¨å±€æœåŠ¡å®ä¾‹ï¼ˆå·²é¢„æ„å»ºç´¢å¼•ï¼‰
        service = get_service_instance()

        if fc_info.action == "startSession":
            logger.info("æ‰§è¡Œå¯åŠ¨ä¼šè¯æ“ä½œ")
            result = service.start_session(fc_info.topic)
            logger.info("Start session success")
            logger.info("==================== å‡½æ•°è°ƒç”¨ç»“æŸ ====================")
            return result

        elif fc_info.action == "getFunctionCallResult":
            if not fc_info.query:
                logger.error("è·å–å‡½æ•°è°ƒç”¨ç»“æœæ—¶ç¼ºå°‘queryå‚æ•°")
                logger.info("==================== å‡½æ•°è°ƒç”¨å¼‚å¸¸ç»“æŸ ====================")
                return {"result": "", "error": "Missing query parameter"}

            logger.info("å¼€å§‹è·å–å‡½æ•°è°ƒç”¨ç»“æœ")
            result = await service.get_function_call_result(fc_info.query, fc_info.topic, fallback_strategy)
            logger.info("Get function call result success")
            logger.info(f"è¿”å›ç»“æœåŒ…å«resultå­—æ®µ: {bool(result.get('result'))}")
            logger.info("==================== å‡½æ•°è°ƒç”¨ç»“æŸ ====================")
            return result

        else:
            logger.error(f"æœªå®šä¹‰çš„åŠ¨ä½œ: {fc_info.action}")
            logger.info("==================== å‡½æ•°è°ƒç”¨å¼‚å¸¸ç»“æŸ ====================")
            return {"result": "", "error": f"Unknown action: {fc_info.action}"}

    except Exception as e:
        logger.error(f"å¤„ç†å®æ—¶å‡½æ•°è°ƒç”¨æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)
        logger.info("==================== å‡½æ•°è°ƒç”¨å¼‚å¸¸ç»“æŸ ====================")
        return {"result": "", "error": f"Internal error: {str(e)}"}
